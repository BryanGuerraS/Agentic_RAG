# ğŸš€ RAG Tradicional con Langchain y GradioğŸŒŸ
Â¡Bienvenido al proyecto! ğŸ‰ Este repositorio contiene una implementaciÃ³n de una API que utiliza **Gradio** para procesar preguntas, buscar texto relevante y generar respuestas utilizando un modelo de lenguaje grande (LLM). La aplicaciÃ³n se integra con **Cohere** para la generaciÃ³n de embeddings, **ChromaDB** para la bÃºsqueda de similitudes y **Gradio** como interfaz para seleccionar el documento y realizar consultas.

## ğŸ§  Principales Insights o Mejoras del Proyecto
- **Framework de Langchain y Cohere para el procesamiento de consultas**: Langchain y Cohere se emplean para el procesamiento avanzado de consultas, optimizando la interacciÃ³n con los documentos y generando respuestas precisas basadas en el contenido cargado.
- **Carga de Archivos mediante Gradio**: La interfaz permite cargar archivos de manera intuitiva usando Gradio, soportando formatos como `.pdf` y `.docx`. Los archivos se indexan automÃ¡ticamente en ChromaDB para realizar consultas.
- **Procesamiento de documentos "docx." y "pdf."**: Se ha implementado un sistema de carga y procesamiento de documentos `.docx` y `.pdf` de forma eficiente. Los archivos se almacenan en una base de datos para facilitar su consulta posterior.
- **Interfaz amigable para usuarios No TÃ©cnicos**: La interfaz estÃ¡ diseÃ±ada para ser simple y accesible, permitiendo que cualquier usuario, sin conocimientos tÃ©cnicos, pueda cargar documentos y hacer consultas de manera fluida.
- **Mejora de Embeddings a multilingual**: Se ha mejorado el sistema de embeddings para ofrecer soporte multilingÃ¼e, permitiendo consultas en varios idiomas y garantizando una mayor versatilidad en el uso global de la herramienta.
- **Mejora de llamado de ChromaDB**: Se ha optimizado la manera en que se interactÃºa con ChromaDB, mejorando la velocidad de indexaciÃ³n y la precisiÃ³n en las bÃºsquedas, lo que resulta en un rendimiento mÃ¡s rÃ¡pido y confiable durante las consultas.


## ğŸŒŸ CaracterÃ­sticas principales
- ğŸ—‚ï¸ Procesamiento de documentos `.docx` y `.pdf` para extraer informaciÃ³n relevante.  
- ğŸ” Almacenamiento de embeddings en **ChromaDB** para bÃºsqueda eficiente de similitudes.  
- ğŸ¤– Respuestas concisas y personalizadas generadas con modelos LLM.  
- ğŸ“¦ Despliegue simplificado con Docker.  


## ğŸ“‚ Estructura del proyecto
```console
ğŸ“ app  
â”œâ”€â”€ cargar_en_chroma_db.py  # Carga y almacenamiento de documentos en ChromaDB.
â”œâ”€â”€ config.py               # GestiÃ³n y validaciÃ³n de variables de entorno.
â”œâ”€â”€ inicializar_db.py       # InicializaciÃ³n y combinaciÃ³n de documentos preprocesados.
â”œâ”€â”€ models.py               # DefiniciÃ³n de los modelos de datos.
â”œâ”€â”€ services.py             # Conjunto de funciones para procesar las consultas.
ğŸ“ Archivos de Referencia
â”œâ”€â”€ Traditional_Rag_vs_Agentic_Rag.gif # Ejemplo de Flujo de proceso en Rag Tradicional y Rag con Agentes.
ğŸ“ chroma_db
â”œâ”€â”€ preprocessed/           # Base de datos de embeddings para documentos preprocesados.
â”œâ”€â”€ uploaded/               # Base de datos de embeddings para documentos cargados.
ğŸ“ documents
â”œâ”€â”€ preprocessed/           # Documentos ya procesados.
â”œâ”€â”€ uploaded/               # Documentos cargados por el usuario.
ğŸ“ images		    # Conjunto de imagenes para el chatbot y de ejemplo.
Dockerfile                  # ConfiguraciÃ³n para construir la imagen Docker.
main.py                     # Archivo principal de la aplicaciÃ³n.
requirements.txt            # LibrerÃ­as requeridas.
```

## ğŸš€ CÃ³mo ejecutar el proyecto
### 1ï¸âƒ£ Requisitos previos
- ğŸ Python 3.9+  
- ğŸ³ Docker instalado.  

### 2ï¸âƒ£ EjecuciÃ³n local
1. Clona el repositorio:
```console
git clone https://github.com/BryanGuerraS/Traditional-RAG-with-Gradio-Upload-Files.git
cd Traditional-RAG-with-Gradio-Upload-Files
```
2. Crea un entorno virtual e instala las dependencias:
```console
python -m venv env
source env/bin/activate  # En Windows: .\venv\Scripts\activate
pip install -r requirements.txt
```

3. Crea un archivo **.env** en la raÃ­z del proyecto con las siguientes claves:
```console
LANGCHAIN_API_KEY=tu_clave_aqui
COHERE_API_KEY=tu_clave_aqui
```

4. Inicia la API
```console
uvicorn main:app --reload
```
5. Prueba la API
Prueba la API en: http://127.0.0.1:7860/

### 3ï¸âƒ£ EjecuciÃ³n con Docker
1. Construye la imagen Docker:
```console
docker build -t rag_gradio .
```
2. Ejecuta el contenedor:
```console
docker run -p 8000:8000 rag_gradio
```
3. Prueba la API:
- La API estarÃ¡ disponible en: http://127.0.0.1:7860/


## ğŸ› ï¸ Endpoints principales
### Principales preguntas:
- Procesa una pregunta y genera una respuesta basado en el documento seleccionado.
![Ejemplo_01](images/img_example_01.jpg)

- Permite cargar archivos nuevos y cambiar el documento seleccionado.
![Ejemplo_02](images/img_example_02.jpg)


## ğŸ“– DocumentaciÃ³n adicional
- Docker: Este proyecto incluye un archivo Dockerfile que permite desplegar la API rÃ¡pidamente en un contenedor.
- MÃ³dulos separados: CÃ³digo modular y bien documentado para facilitar su mantenimiento y escalabilidad.

## ğŸŒ Contribuciones
Â¡Las contribuciones son bienvenidas! Si encuentras un problema o deseas agregar una mejora, por favor abre un issue o envÃ­a un pull request. ğŸ™Œ